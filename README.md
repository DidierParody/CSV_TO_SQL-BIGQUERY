---  

```md
# ğŸŒ ETL de Datos ClimÃ¡ticos con Python y BigQuery  

Este proyecto implementa un **flujo ETL (ExtracciÃ³n, TransformaciÃ³n y Carga)** para recopilar, limpiar y almacenar datos climÃ¡ticos en Google BigQuery. Se obtienen datos de temperatura, humedad y velocidad del viento desde la API de Open-Meteo, se procesan con Pandas y se almacenan en una base de datos en la nube.  

## ğŸš€ TecnologÃ­as utilizadas  

- **Python**: Lenguaje de programaciÃ³n principal  
- **httpx**: Para hacer solicitudes HTTP de manera asÃ­ncrona  
- **pandas**: Para manipulaciÃ³n y transformaciÃ³n de datos  
- **google-cloud-bigquery**: Para interactuar con BigQuery  
- **dotenv**: Para la gestiÃ³n de variables de entorno  

## ğŸ“ Estructura del Proyecto  

```
ETL/
â”‚â”€â”€ extraction.py      # Obtiene datos climÃ¡ticos de Open-Meteo
â”‚â”€â”€ transformation.py  # Convierte y limpia los datos en CSV
â”‚â”€â”€ load.py            # Sube los datos transformados a BigQuery
â”‚â”€â”€ main.py            # Ejecuta todo el flujo ETL
â”‚â”€â”€ .env               # Variables de entorno (no subir a GitHub)
â”‚â”€â”€ requirements.txt   # Dependencias del proyecto
â”‚â”€â”€ README.md          # DocumentaciÃ³n
```

## ğŸ› ï¸ ConfiguraciÃ³n  

### 1ï¸âƒ£ Clonar el repositorio  
```bash
git clone https://github.com/tuusuario/etl-climatico.git
cd etl-climatico
```

### 2ï¸âƒ£ Crear un entorno virtual e instalar dependencias  
```bash
python3 -m venv etlenv
source etlenv/bin/activate  # En Windows: etlenv\Scripts\activate
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configurar variables de entorno  
Crea un archivo **.env** y define las siguientes variables:  

```
CREDENTIALS_PATH=/ruta/a/credenciales.json
TABLE_ID=etl-452119.dataset.tabla
```

## ğŸ“Œ Flujo de Datos  

1ï¸âƒ£ **ExtracciÃ³n (`extraction.py`)**  
   - Obtiene datos meteorolÃ³gicos de ciudades como BogotÃ¡, MedellÃ­n, etc.  
   - Usa `httpx` para hacer solicitudes asÃ­ncronas.  
   - Guarda los datos en formato JSON.  

2ï¸âƒ£ **TransformaciÃ³n (`transformation.py`)**  
   - Convierte el JSON a CSV usando `pandas`.  
   - Expande los datos en mÃºltiples filas para facilitar su anÃ¡lisis.  

3ï¸âƒ£ **Carga (`load.py`)**  
   - Sube el CSV final a Google BigQuery.  
   - Utiliza `google-cloud-bigquery` para la conexiÃ³n.  

## â–¶ï¸ EjecuciÃ³n  

Para ejecutar todo el proceso ETL, usa el siguiente comando:  
```bash
python3 main.py
```

## ğŸ“Š VisualizaciÃ³n de los datos en BigQuery  

1. Ve a [Google BigQuery Console](https://console.cloud.google.com/bigquery)  
2. Selecciona tu dataset y tabla  
3. Usa una consulta SQL como:  
   ```sql
   SELECT * FROM `etl-452119.dataset.tabla` LIMIT 10;
   ```

## âœ¨ Mejoras Futuras  

- Agregar almacenamiento en una base de datos local como PostgreSQL.  
- Crear un dashboard en **Google Looker Studio** para visualizar los datos.  
- Optimizar la transformaciÃ³n con Apache Spark para datasets grandes.  

---

ğŸ“Œ **Autor**: Didier JosÃ© Torres Parodis  
ğŸ“† **Ãšltima actualizaciÃ³n**: Febrero 2025  
ğŸ“‚ **Licencia**: MIT  
```

---

### ğŸ”¹ Â¿QuÃ© hace esta documentaciÃ³n bien?  

âœ… **ExplicaciÃ³n gradual**: Comienza sencilla y se vuelve tÃ©cnica.  
âœ… **Estructura clara**: IntroducciÃ³n, tecnologÃ­as, configuraciÃ³n, flujo, ejecuciÃ³n.  
âœ… **Ejemplos y comandos**: Para que cualquier usuario pueda ejecutarlo fÃ¡cilmente.  
âœ… **Mejoras futuras**: Para mostrar posibles optimizaciones.  
